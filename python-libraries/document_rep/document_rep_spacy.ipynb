{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document_rep_spacy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF27vN-7alO2"
      },
      "source": [
        "#!/usr/bin/python"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iV0I7X0amWe",
        "outputId": "54661821-ebd0-4b25-bd36-c5e4872354d5"
      },
      "source": [
        "!git clone https://github.com/st1800eafit/st1800_20211.git\r\n",
        "!pip install spacy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'st1800_20211' already exists and is not an empty directory.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfDrwCWKapDW"
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "import re \r\n",
        "import spacy"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_VO3T46asdx"
      },
      "source": [
        "# Importamos los documentos de entrada para trabajar\r\n",
        "files_location = os.path.join(\"/\",\"content\",\"st1800_20211\",\"datasets\", \"papers_sample_pdf/\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY3QzJT6auvp"
      },
      "source": [
        "# Abrimos los documentos con extensiones .txt. Una vez tenemos todos los nombres de los documentos, los abrimos y adicionamos a la variable corpus\r\n",
        "filenames = glob.glob(files_location+\"*.txt\")\r\n",
        "corpus = \"\"\r\n",
        "for file in filenames:\r\n",
        "  corpus += open(file, \"r\").read()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT-DFcf-axJG"
      },
      "source": [
        "# Abrimos el .xml. Primero tenemos que limpiar el xml de etiquetas, para luego agregarlo al corpus. En este momento tenemos al corpus totalmente actualizado\r\n",
        "filename_xml = glob.glob(files_location+\"*.xml\")\r\n",
        "xml_file = open (filename_xml[0], \"r\").read()\r\n",
        "xml_file = re.sub('<[^>]*>', \"\", xml_file)\r\n",
        "corpus += xml_file"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNnGrY9yay5j",
        "outputId": "2e27dff1-35b4-48c9-c0d8-bfe024073735"
      },
      "source": [
        "# Ahora utilizamos spacy para tokenizar el corpus\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "nlp.max_length = 2000000 \r\n",
        "pre_corpus = nlp(corpus)\r\n",
        "\r\n",
        "#Imprimimos los tokens extraidos por spacy\r\n",
        "print ([pre_corpus[i].text for i in range(25)])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '\\n\\n', 'Variations', 'on', 'a', 'theme', 'by', 'Schalkwijk', 'and', 'Kailath', '\\n', 'Robert', 'G.', 'Gallager', '\\n\\n', 'Barış', 'Nakiboğlu', '\\n\\n', 'arXiv:0812.2709v4', '[', 'cs', '.', 'IT', ']', '20']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF0tE9sGbZlv",
        "outputId": "fab60cbb-a91b-473a-cfbd-bfe10d7eb7c2"
      },
      "source": [
        "# Ahora removemos los stop words\r\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\r\n",
        "\r\n",
        "no_stopword_corpus = [token for token in pre_corpus if not token.is_stop and not token.is_punct]\r\n",
        "\r\n",
        "print ([no_stopword_corpus[i].text for i in range(25)])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '\\n\\n', 'Variations', 'theme', 'Schalkwijk', 'Kailath', '\\n', 'Robert', 'G.', 'Gallager', '\\n\\n', 'Barış', 'Nakiboğlu', '\\n\\n', 'arXiv:0812.2709v4', 'cs', '20', 'Nov', '2009', '\\n\\n', 'Abstract', '\\n', 'Schalkwijk', 'Kailath', '1966']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Y1o6hKcqTM",
        "outputId": "bfb0928f-4150-45e7-d2ff-7cefa6adbf3d"
      },
      "source": [
        "# Aplicamos TF\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "tf = Counter(no_stopword_corpus)\r\n",
        "\r\n",
        "# Palabras comunes\r\n",
        "common_words = tf.most_common(5)\r\n",
        "print (common_words)\r\n",
        "\r\n",
        "# Palabras Unicas\r\n",
        "unique_words = [word for (word, freq) in tf.items() if freq == 1]\r\n",
        "print (unique_words[:10])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 1), (\n",
            "\n",
            ", 1), (Variations, 1), (theme, 1), (Schalkwijk, 1)]\n",
            "[1, \n",
            "\n",
            ", Variations, theme, Schalkwijk, Kailath, \n",
            ", Robert, G., Gallager]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}